{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled64.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMULGm0sNw5CmkluY+TtSuf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/KOSA-Python_Algorithm/blob/main/concurrent/Faster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGlefKfDg1aO",
        "outputId": "897a2d44-d9fc-4dd3-9797-58c14c762552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">Saved https://docs.python.org/3/library/concurrent.futures.html to ./concurrent.futures.html\n",
            ">Saved https://docs.python.org/3/library/queue.html to ./queue.html\n",
            ">Saved https://docs.python.org/3/library/concurrency.html to ./concurrency.html\n",
            ">Saved https://docs.python.org/3/library/multiprocessing.shared_memory.html to ./multiprocessing.shared_memory.html\n",
            ">Saved https://docs.python.org/3/library/contextvars.html to ./contextvars.html\n",
            ">Saved https://docs.python.org/3/library/threading.html to ./threading.html\n",
            ">Saved https://docs.python.org/3/library/concurrent.html to ./concurrent.html\n",
            ">Saved https://docs.python.org/3/library/multiprocessing.html to ./multiprocessing.html\n",
            ">Saved https://docs.python.org/3/library/subprocess.html to ./subprocess.html\n",
            ">Saved https://docs.python.org/3/library/sched.html to ./sched.html\n"
          ]
        }
      ],
      "source": [
        "# SuperFastPython.com\n",
        "# download document files concurrently and save the files locally concurrently\n",
        "from os import makedirs\n",
        "from os.path import basename\n",
        "from os.path import join\n",
        "from urllib.request import urlopen\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from concurrent.futures import as_completed\n",
        "\n",
        "# download a url and return the raw data, or None on error\n",
        "def download_url(url):\n",
        "    try:\n",
        "        # open a connection to the server\n",
        "        with urlopen(url, timeout=3) as connection:\n",
        "            # read the contents of the html doc\n",
        "            return (connection.read(), url)\n",
        "    except:\n",
        "        # bad url, socket timeout, http forbidden, etc.\n",
        "        return (None, url)\n",
        "\n",
        "# save data to a local file\n",
        "def save_file(url, data, path):\n",
        "    # get the name of the file from the url\n",
        "    filename = basename(url)\n",
        "    # construct a local path for saving the file\n",
        "    outpath = join(path, filename)\n",
        "    # save to file\n",
        "    with open(outpath, 'wb') as file:\n",
        "        file.write(data)\n",
        "    return outpath\n",
        "\n",
        "# download a list of URLs to local files\n",
        "def download_docs(urls, path):\n",
        "    # create the local directory, if needed\n",
        "    makedirs(path, exist_ok=True)\n",
        "    # create the thread pool\n",
        "    n_threads = len(urls)\n",
        "    with ThreadPoolExecutor(n_threads) as executor:\n",
        "        # download each url and save as a local file\n",
        "        futures = [executor.submit(download_url, url) for url in urls]\n",
        "        # process each result as it is available\n",
        "        for future in as_completed(futures):\n",
        "            # get the downloaded url data\n",
        "            data, url = future.result()\n",
        "            # check for no data\n",
        "            if data is None:\n",
        "                print(f'>Error downloading {url}')\n",
        "                continue\n",
        "            # save the data to a local file\n",
        "            outpath = save_file(url, data, path)\n",
        "            # report progress\n",
        "            print(f'>Saved {url} to {outpath}')\n",
        "\n",
        "# python concurrency API docs\n",
        "URLS = ['https://docs.python.org/3/library/concurrency.html',\n",
        "        'https://docs.python.org/3/library/concurrent.html',\n",
        "        'https://docs.python.org/3/library/concurrent.futures.html',\n",
        "        'https://docs.python.org/3/library/threading.html',\n",
        "        'https://docs.python.org/3/library/multiprocessing.html',\n",
        "        'https://docs.python.org/3/library/multiprocessing.shared_memory.html',\n",
        "        'https://docs.python.org/3/library/subprocess.html',\n",
        "        'https://docs.python.org/3/library/queue.html',\n",
        "        'https://docs.python.org/3/library/sched.html',\n",
        "        'https://docs.python.org/3/library/contextvars.html']\n",
        "# local path for saving the files\n",
        "PATH = './'\n",
        "# download all docs\n",
        "download_docs(URLS, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gDx4vcGRhCPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}